dump_dir: /workspace/bci/checkpoints/bci/bci_AY2l_ablation5
name: bci_AY2l_ablation5
steps: 50_000 # (CW) was 100_000 # This controls both LR schedule and Total # of steps. [173k steps for 1 epoch on TUH w/ 4GPUs.]
grad_acc_steps: 1 # (CW) - wasnt here. Defaults to 1. 
probe_freq: null
seed: 333     # (CW) - This is used for something other than data shuffling. Not sure what.
repa_into_encoder: false
repa_into_decoder: false
load_distillation_model: false

channel_loss_weighting: false
distill_into_encoder: false
track_dataset_stats: false # (CW) - added this to track data stats in training loop. Defaults to false.

encoder_mmd_weight: 0 #1e-2 #1e-4 # (CW) - set to zero to try to fix GradNorm Spikes. (was 1e-4 orig) (has been 1e-2).

optim:      # (CW) - Note: Using Cosine with Warmup == 1Cycle basically. 
  lr: 1e-4  # (CW) - was 3e-4
  weight_decay: 0.1 # (CW) - was 0.01 # Moh suggests 0.1
  warmup: 2000 # (CW) is 10% of 1 epoch - was 2000 # Beren suggested lower.
  lr_min_ratio: 1e-2 #0.000001
  clip: 200.0 #10.0 # 0.1 (CW) - was 1.0 orig. 0.1 for OpenNeuro
  beta1: 0.9 #0.85 # (CW) - wasnt here. Defaults to 0.9 - Moh suggestion (0.85) for stability.
  beta2: 0.95 #0.9 # (CW) - wasnt here. Defaults to 0.95 - Moh suggestion (0.9) for stability.
  scheduler: cosine # (CW) - was trapezoidal
  # optim_cls: cadamw # (CW) Note: This is "Cautious Adam", not CAAdamW! Dont use.

distributed:
  fsdp_type: no_shard 
  compile: true
  model_dtype: bf16 # fp32 # (CW) -  was bf16 # 
  matmul_allow_tf32: true # (CW) - TRY SETTING TO TRUE. A DATATYPE LIKE BF16. SHOULD SPEED THINGS UP.
  selective_activation_checkpointing: false # (CW) - THIS WILL NOT WORK WITH FLEX ATTENTION.
  tp_size: 1
  spawn_method: spawn # added by (CW). - Was not here and defaulted to "forkserver"
  # dp_replicate: 1     # added by (CW). - turn off FSDP to run on 1 GPU. Comment out to run on multi-GPU
  # dp_shard: 1         # added by (CW). - turn off FSDP to run on 1 GPU. Comment out to run on multi-GPU

model:
  dim: 1024
  n_layers: 16
  head_dim: 64
  seqlen_t: false
  huber_c: null                              # (CW) - was 0.1 [null uses MSE only] (value was tuned for audio data anyway)
  input_dim: 32 #128 #131                        # (CW) - set to num time points when not mixing chans (or |tf|+3 if mixing chans with xyz)
  encoder_input_dim: 32 #128 #131                # (CW) - set to num time points when not mixing chans   
  encoder_output_dim: 32 #128 #131 # input_dim*2 # (CW) - "EOD" effects enc_out dim2 - the "registers" [B, T/ds, EOD]
  encoder_latent_downsample_factor: 1        # (CW) - effects the number / time-dimension of "registers" in the encoder and enc_out dim1 [B, T/ds, EOD]

  encoder_sliding_window: 65536 #2048 #256 #512 #64 #128           # (CW) - default is 128
  sliding_window: 65536 #2048 #256 #64 #128                   # (CW) - default is 128
  xattn_sliding_window: 65536 #2048 #256 #64 #32              # (CW) - default is 32
  max_seqlen: 50 #100 #40 #10 # 256 #42000 #34000 #66000 #16384                      # (CW) - trying to allow ROPE to work with longer seqs when encoder_latent_downsample_factor=1. 

  stft_global_sigma: 0.1 # defaults to 1.0 # (CW) - global sigma for z initialization in model.sample .

  # decoder_encoder_dropout: 0.1          # 0.1 # (CW) - prob of dropping out entire batch in encoder output going into decoder. (default is 0.1)
  # decoder_timestep_dropout: 0.2         # 0.1 # (CW) - prob of dropping out channels from encoder output going into decoder if DONT_MIX_CHANNELS=True. (default is 0.1)

  adaptive_loss_weighting: true  # (CW) - set to true to try to fix GradNorm Spikes.
  num_fine_time_pts: 32 #128
  rope_dim: 4 # 0 = NoPE, 1 = 1D-RoPE in seq-dim, 4 = 4D-RoPE in {x,y,z,tc}.
  rope_theta: 10000.0 # Default = 10000.0 - is too big when max_seqlen=40?
  tok_idx_type: "{x,y,z,tc}" # null, "t_coarse", "chan_id", "stack_arange_seqlen", "{x,y,z,tc}"
  dont_noise_chan_xyz: false # If true, do not add noise to channel {x,y,z}-position in EncoderDecoder.sample (use in tandem with NoPE)
  dropout_type: "learnable" # {"zero", "learnable"}
  
  # ffn_dim_multiplier: 0.7
  # encoder_hidden_dim: 2048

  # decoder_repa_index: 3
  # encoder_repa_index: 10
  # repa_dim: 768
  # repa_loss_fn: huberweighted

data:
  use_b2: true # If true, use Backblaze B2 for dataset loading, otherwise use local filesystem.

  # data_dir: /workspace/bci/data/pt3d/v3/train/tuh
  # data_dir: /workspace/bci/data/pt3d/v3/train/one
  # data_dir: /workspace/bci/data/pt3d/v3/train
  # data_dir: /workspace/bci/data/pt3d/v3_short/train/tuh
  # data_dir: /workspace/bci/data/pt3d/v4/train/tuh
  # data_dir: /workspace/bci/data/pt3d/v4/train
  # data_dir: /workspace/bci/data/pt3d/v5/train     # training dataset locally.
  # data_dir: datasets/v5/train/                    # training dataset on B2. 
  data_dir: datasets/v5/train/tuh/                  # overfit dataset on B2 (with glob filter="**/*_06664*.pt"). 
  glob_filter: "**/*_06664*.pt" #"**/*.pt" # this filters the dataset to only contain files with this string in it.

  chan_num_filter: null # null or integer number of channels we want in each sample
  sample_rate: 256 #512 # datasets are resampled to 256Hz now.
  seq_len: 1280 #2560   # dataset are fixed to 1280 timepoints now 5 sec at 256Hz.
  num_fine_time_pts: 32 #128
  use_coarse_time: "B" # How to chop signals in to coarse-time, fine-time & channels using chop_and_reshape_signals or chop_signals_only
  cat_chan_xyz_and_eeg: false
  dont_noise_chan_xyz: false # If true, do not add noise to channel {x,y,z}-position in EEGProcessor.process (use in tandem with NoPE)
  randomly_permute_sequence: true
  stft_global_sigma: 0.1 # defaults to 1.0 # (CW) - global sigma for stft noise schedule.
  masked_in_decoder: false # If true, mask out channels in decoder input when channel is dropped. (true works, false does not)
  channel_dropout_prob: 0.90 # Probability of applying channel dropout to the EEG signal during training.

  num_bins_discretize_xyz_chan_pos: 50 #100 # Number of bins to discretize channel positions to use in 4d-RoPE. # 40 with "old" xyz_extremes, 100 with "thirteens" xyz_extremes
  chan_pos_xyz_extremes_type: "twelves" # "thirteens" # "old" for v4 dataset or "thirteens" for v5 dataset
  
  # data_norm: 1 #5 # (CW) - this is the norm to divide the data by, to normalize it to [-1,1] range.
  batch_size: 1 #64 #1024 # 32              # (CW) - editted this for debug. KEEP AT 1 NOW.
  target_packed_seqlen: 15000 #16384 #   32768 #         # (CW) - Target seqlen for packed samples. This is essentially batch size.
  # do_N_epochs: None
  num_workers: 8 #16              # (CW) - editted this for debug. TRY DOUBLING THIS
  prefetch_factor: 8          # (CW) - editted this for debug. TRY DOUBLING THIS.
  persistent_workers: true    # (CW) - added this. defaults to true
  pin_memory: true            # (CW) - added this. defaults to true
  diffusion_forcing: false
  shuffle: true               # (CW) - trying to pin down is bad data is causing GradNorm explosion.
  seed: 316                   # (CW) - seed for data shuffling (int or null)

checkpoint:
  dump:
    every: 1000 #10000
    keep: 200 #35
  eval:
    every: 1000 # 500000000000000 # How often to run eval harness.
    keep: -1
  load_optimizer_state: true # (CW) - set to false to not load optimizer state from checkpoint, defaults to true. (CW)
  # init_ckpt_path: /workspace/AY2latent/lingua/capella_forcing_repa2_fmask_new_data_fat_bal_extreme/checkpoints/0000520000
  # init_ckpt_path: /workspace/bci/checkpoints/bci/bci_AY2l_bigrun15i/checkpoints/0000070000
  # init_ckpt_path: /workspace/bci/checkpoints/bci/bci_AY2l_bigrun15/checkpoints/0000050000
  # init_ckpt_path: /workspace/bci/checkpoints/bci/bci_AY2l_bigrun15r/checkpoints/0000055000
  # init_ckpt_path: /workspace/bci/checkpoints/bci/bci_AY2l_bigrun16e/checkpoints/0000150000

logging:
  freq: 1
  wandb:
    project: "ay2l_bci_cw"  # Add your project name

# async_eval_gpus: 8

eval:
  use_b2: true # If true, use Backblaze B2 for dataset loading, otherwise use local filesystem.

  # data_dir: /workspace/bci/data/pt3d/v3/train/tuh
  # data_dir: /workspace/bci/data/pt3d/v3/eval/tuh
  # data_dir: /workspace/bci/data/pt3d/v3/eval/one
  # data_dir: /workspace/bci/data/pt3d/v3/eval
  # data_dir: /workspace/bci/data/pt3d/v3/train
  # data_dir: /workspace/bci/data/pt3d/v4/train/tuh
  # data_dir: /workspace/bci/data/pt3d/v4/train
  # data_dir: /workspace/bci/data/pt3d/v5/train
  # data_dir: datasets/v4/train/
  # data_dir: /workspace/bci/data/eval_datasets/eval_datasets_tuh/dreamer/labels
  # data_dir: /workspace/bci/data/eval_datasets/eval_datasets_tuh/faced/labels
  # data_dir: /workspace/bci/data/eval_datasets_chunked/eval_datasets_tuh/moabb2015/labels/
  # data_dir: /workspace/bci/data/eval_datasets_chunked/eval_datasets_tuh/moabb2016/labels/
  # data_dir: /workspace/bci/data/eval_datasets_chunked/eval_datasets_tuh/moabb2017/labels/

  # data_dir: datasets/v5/train/                    # training dataset on B2. 
  data_dir: datasets/v5/train/tuh/                  # overfit dataset on B2 (with glob filter="**/*_06664*.pt"). 
  glob_filter: "**/*_06664*.pt" #"**/*.pt" # this filters the dataset to only contain files with this string in it.

  chan_num_filter: null #null or integer number of channels we want in each sample
  sample_rate: 256 #512 # # datasets are resampled to 256Hz now.
  seq_len: 1280 #2560 #   # dataset are fixed to 1280 timepoints now 5 sec at 256Hz.
  num_fine_time_pts: 32 #128
  use_coarse_time: "B" # How to chop signals in to coarse-time, fine-time & channels using chop_and_reshape_signals or chop_signals_only
  cat_chan_xyz_and_eeg: false
  dont_noise_chan_xyz: false # If true, do not add noise to channel {x,y,z}-position in EEGProcessor.process (use in tandem with NoPE)
  randomly_permute_sequence: false
  stft_global_sigma: 0.1 # defaults to 1.0 # (CW) - global sigma for stft noise schedule.
  channel_dropout_prob: 0.90 # Probability of applying channel dropout to the EEG signal during training.
  masked_in_decoder: false # If true, mask out channels in decoder input when channel is dropped. (true works, false does not)

  num_bins_discretize_xyz_chan_pos: 50 #100 # Number of bins to discretize channel positions to use in 4d-RoPE. # 40 with "old" xyz_extremes, 100 with "thirteens" xyz_extremes
  chan_pos_xyz_extremes_type: "twelves" #"thirteens" # "old" for v4 dataset or "thirteens" for v5 dataset

  # data_norm: 1 #5 # (CW) - this is the norm to divide the data by, to normalize it to [-1,1] range.
  num_batches: 5 #50
  batch_size: 1 # 650 #32              # (CW) - editted this for debug.
  target_packed_seqlen: 15000 # 4096 # 32768    # (CW) - Target seqlen for packed samples. This is essentially batch size.
  # do_N_epochs: 1
  num_workers: 8 #32             # (CW) - editted this for debug. TRY DOUBLING THIS
  prefetch_factor: 8          # (CW) - editted this for debug. TRY DOUBLING THIS.
  persistent_workers: true    # (CW) - added this. defaults to true
  pin_memory: true            # (CW) - added this. defaults to true
  diffusion_forcing: false
  shuffle: true               # (CW) - trying to pin down is bad data is causing GradNorm explosion.
  seed: 316                   # (CW) - seed for data shuffling (int or null)



# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #
# 
# profiling: # (CW) deleted to avoid pain of installing xformers
#   run: False
#   mem_warmup: 0
#   mem_steps: 4
#   profile_warmup: 100
#   profile_steps: 4
