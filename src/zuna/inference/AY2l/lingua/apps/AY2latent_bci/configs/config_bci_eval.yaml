model:
  # NOTE MODEL IS ACTUALLY LOADED FROM JSON CONFIG FILE IN HUGGINGFACE REPO.
  dim: 1024
  n_layers: 16
  head_dim: 64
  seqlen_t: false
  huber_c: null # [null uses MSE]
  input_dim: 32 
  encoder_input_dim: 32 
  encoder_output_dim: 32 
  encoder_latent_downsample_factor: 1   

  encoder_sliding_window: 65536 # big enough to not be used
  sliding_window: 65536 
  xattn_sliding_window: 65536 
  max_seqlen: 50                 # used with ROPE - tc and discretized channel positions.

  adaptive_loss_weighting: true  
  num_fine_time_pts: 32 
  rope_dim: 4 # 0 = NoPE, 1 = 1D-RoPE in seq-dim, 4 = 4D-RoPE in {x,y,z,tc}.
  rope_theta: 10000.0
  tok_idx_type: "{x,y,z,tc}"  # null, "t_coarse", "chan_id", "stack_arange_seqlen", "{x,y,z,tc}" 
  stft_global_sigma: 0.1 # Noising process std - should match signal std.
  dropout_type: "zeros" # {"zero", "learnable"}


data:
  # data_dir: ./tutorials/data/2_pt_input/      # Input .pt files to be processed.
  data_dir: /data/datasets/bci/dataset_downloads_cw/pip_test/working/2_pt_input
  # data_dir: /data/datasets/bci/pip_test_2
  export_dir: ./tutorials/data/3_pt_output/   # Where to save output .pt files after processing.

  glob_filter: "**/*.pt" # this filters the dataset to only contain files with this string in it.
  chan_num_filter: null # null or integer number of channels we want in each sample (another filter on input data)
  sample_rate: 256
  seq_len: 1280 # number of timepoints in each sample (fixed 5 sec samples at 256 Hz)
  num_fine_time_pts: 32 
  use_coarse_time: "B" # How to chop signals in to coarse-time, fine-time & channels using chop_and_reshape_signals.
  randomly_permute_sequence: false
  channel_dropout_prob: 0.0 # Probability of applying channel dropout to the EEG signal during training if float. of downsampling grid if int
  stft_global_sigma: 0.1 # Noising process std - should match signal std.

  num_bins_discretize_xyz_chan_pos: 50 # Number of bins to discretize channel positions to use in 4d-RoPE. (we used 50)
  chan_pos_xyz_extremes_type: "twelves" 

  data_norm: 10.0 # this is the norm to divide the data by, to normalize it.
  data_clip: 1.0  # Clip data to this value after normalization.
  target_packed_seqlen: 100000            # Target seqlen for packed samples. This is essentially batch size.

  # For dataloader.
  num_workers: 0              
  prefetch_factor: null          
  persistent_workers: false    
  pin_memory: false     

  shuffle: true              
  seed: 316                   
  
dump_dir: zuna_log/ 